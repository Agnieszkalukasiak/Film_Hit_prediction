{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "*  Fit and evaluate a classification model to predict if a prospect will churn or not.\n",
    "\n",
    "## Inputs\n",
    "\n",
    "* outputs/datasets/collection/TelcoCustomerChurn.csv\n",
    "* Instructions on which variables to use for data cleaning and feature engineering. They are found in each respective notebook.\n",
    "\n",
    "## Outputs\n",
    "\n",
    "* Train set (features and target)\n",
    "* Test set (features and target)\n",
    "* Data cleaning and Feature Engineering pipeline\n",
    "* Modeling pipeline\n",
    "* Feature importance plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the working directory from its current folder to its parent folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/Film_Hit_prediction/jupyter_notebooks'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the parent of the current directory the new current directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You set a new current directory\n"
     ]
    }
   ],
   "source": [
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"You set a new current directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/Film_Hit_prediction'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shapes:\n",
      "X_train shape: (3284, 269)\n",
      "y_train shape: (3284,)\n",
      "X_test shape: (813, 269)\n",
      "y_test shape: (813,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Load the splits for modeling\n",
    "X_train = pd.read_pickle('/workspace/Film_Hit_prediction/jupyter_notebooks/outputs/engineered/X_train.pkl')\n",
    "X_test = pd.read_pickle('/workspace/Film_Hit_prediction/jupyter_notebooks/outputs/engineered/X_test.pkl')\n",
    "y_train = pd.read_pickle('/workspace/Film_Hit_prediction/jupyter_notebooks/outputs/engineered/y_train.pkl')\n",
    "y_test = pd.read_pickle('/workspace/Film_Hit_prediction/jupyter_notebooks/outputs/engineered/y_test.pkl')\n",
    "\n",
    "\n",
    "print(\"Dataset shapes:\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current split ratio:\n",
      "Training samples: 3284 (80.16%)\n",
      "Test samples: 813 (19.84%)\n"
     ]
    }
   ],
   "source": [
    "print(\"Current split ratio:\")\n",
    "print(f\"Training samples: {X_train.shape[0]} ({X_train.shape[0]/(X_train.shape[0] + X_test.shape[0]):.2%})\")\n",
    "print(f\"Test samples: {X_test.shape[0]} ({X_test.shape[0]/(X_train.shape[0] + X_test.shape[0]):.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: ML Pipeline with all data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Pipeline for Modelling and Hyperparameter Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Define models\n",
    "\n",
    "models = {\n",
    "    'Linear Regression': {\n",
    "        'model': LinearRegression(),\n",
    "        'params': {}\n",
    "    },\n",
    "    'Ridge': {\n",
    "        'model': Ridge(random_state=42),\n",
    "        'params': {\n",
    "            'alpha': [0.01, 0.1, 1.0, 10.0, 100.0],\n",
    "            'solver': ['auto', 'svd', 'cholesky']\n",
    "        }\n",
    "    },\n",
    "    'Lasso': {\n",
    "        'model': Lasso(random_state=42, max_iter=50000, tol=0.1),  \n",
    "        'params': {\n",
    "            'alpha': [1.0 ],  \n",
    "            'selection': ['random'],  \n",
    "            'warm_start': [True, False]\n",
    "        }\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'model': RandomForestRegressor(random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [100, 200],\n",
    "            'max_depth': [10, 20, None],\n",
    "            'min_samples_split': [2, 5],\n",
    "            'n_jobs': [-1]\n",
    "        }\n",
    "    },\n",
    " \n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "\n",
      "Training Linear Regression...\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "\n",
      "==================================================\n",
      "\n",
      "Training Ridge...\n",
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n",
      "\n",
      "==================================================\n",
      "\n",
      "Training Lasso...\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/.pip-modules/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8.381944803156962e+18, tolerance: 7.686652213460292e+18\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/workspace/.pip-modules/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7.859611978441474e+18, tolerance: 7.066407902412163e+18\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import joblib\n",
    "\n",
    "results = {}\n",
    "best_models = {}\n",
    "\n",
    "for name, model_info in models.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "\n",
    "    # GridSearchCV performs the hyperparameter optimization\n",
    "    grid_search = GridSearchCV(\n",
    "        model_info['model'],\n",
    "        model_info['params'],\n",
    "        cv=5,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs= 2 ,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Fit the model\n",
    "    grid_search.fit(X_train, y_train.ravel())\n",
    "    best_models[name] = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import joblib\n",
    "\n",
    "\n",
    "for name, model in best_models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "\n",
    "     # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'R2 Score': r2,\n",
    "        'Best Parameters': grid_search.best_params_\n",
    "    }\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nResults for {name}:\")\n",
    "    print(f\"RMSE: {rmse:.2f}\")\n",
    "    print(f\"R2 Score: {r2:.2f}\")\n",
    "    print(f\"Best Parameters: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Best Performing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "\n",
    "# 1. First identify the best model\n",
    "best_model_name = min(results, key=lambda x: results[x]['RMSE'])\n",
    "best_model = best_models[best_model_name]\n",
    "\n",
    "print(f\"\\nBest performing model: {best_model_name}\")\n",
    "print(f\"Best model RMSE: {results[best_model_name]['RMSE']:.2f}\")\n",
    "\n",
    "# 2. Set up the save directory\n",
    "save_dir = '/workspace/Film_Hit_prediction/jupyter_notebooks/outputs/models'\n",
    "os.makedirs(save_dir, exist_ok=True)  # Create directory if it doesn't exist\n",
    "\n",
    "# 3. Create the filename with model name and date\n",
    "from datetime import datetime\n",
    "current_date = datetime.now().strftime(\"%Y%m%d\")\n",
    "model_filename = f'film_revenue_model_{best_model_name}_{current_date}.joblib'\n",
    "full_path = os.path.join(save_dir, model_filename)\n",
    "\n",
    "# 4. Save the model\n",
    "joblib.dump(best_model, full_path)\n",
    "\n",
    "print(f\"\\nBest model saved as: {model_filename}\")\n",
    "print(\"Model and scalers saved successfully!\")\n",
    "print(\"Best model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the saved Random Forest model\n",
    "model_path = '/workspace/Film_Hit_prediction/jupyter_notebooks/outputs/models/film_revenue_model.joblib'\n",
    "loaded_model = joblib.load(model_path)\n",
    "\n",
    "print(\"Loaded model type:\", type(loaded_model))\n",
    "\n",
    "\n",
    "# Create a DataFrame for feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': loaded_model.feature_importances_\n",
    "})\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print top 10 most important features\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(feature_importance.head(10))\n",
    "\n",
    "# Plot the top 10 feature importance\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(feature_importance['Feature'][:10], feature_importance['Importance'][:10])\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title('Top 10 Feature Importance for Revenue Prediction')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance Score')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to make predictions for new movies based on parameters:\n",
    "-  budget (float): Movie budget in dollars\n",
    "- language (str): Original language (e.g., 'en' for English)\n",
    "- genres (list): List of genres (e.g., ['Action', 'Adventure'])\n",
    "    \n",
    " Returns:\n",
    "- float: Predicted revenue and profit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# First load all necessary models and data\n",
    "def load_models_and_data():\n",
    "    \"\"\"\n",
    "    Load all necessary models, encoders, and data files\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Loading models and data...\")\n",
    "        \n",
    "        # Load the trained model\n",
    "        print(\"Loading revenue prediction model...\")\n",
    "        model = joblib.load('/workspace/Film_Hit_prediction/jupyter_notebooks/outputs/models/film_revenue_model.joblib')\n",
    "        print(\"Model loaded successfully!\")\n",
    "        \n",
    "        # Load the training and test data\n",
    "        print(\"Loading training and test data...\")\n",
    "        with open('/workspace/Film_Hit_prediction/jupyter_notebooks/outputs/engineered/train_df_engineered.pkl', 'rb') as f:\n",
    "            train_df = pickle.load(f)\n",
    "            \n",
    "        with open('/workspace/Film_Hit_prediction/jupyter_notebooks/outputs/engineered/test_df_engineered.pkl', 'rb') as f:\n",
    "            test_df = pickle.load(f)\n",
    "            \n",
    "        # Load encoders and filters\n",
    "        print(\"Loading encoders...\")\n",
    "        with open('/workspace/Film_Hit_prediction/jupyter_notebooks/outputs/cleaned/encoders_and_filters.pkl', 'rb') as f:\n",
    "            encoders = pickle.load(f)\n",
    "\n",
    "        # Load feature engineering data\n",
    "        print(\"Loading feature engineering data...\")\n",
    "        with open('/workspace/Film_Hit_prediction/jupyter_notebooks/outputs/engineered/feature_engineering_data.pkl', 'rb') as f:\n",
    "            eng_data = pickle.load(f)\n",
    "\n",
    "        with open('/workspace/Film_Hit_prediction/jupyter_notebooks/outputs/engineered/all_features.pkl', 'rb') as f:\n",
    "            all_features = pickle.load(f)\n",
    "            features = {col: 0 for col in all_features}\n",
    "\n",
    "        with open('/workspace/Film_Hit_prediction/jupyter_notebooks/outputs/engineered/feature_scaler.pkl', 'rb') as f:\n",
    "            scaler = pickle.load(f)\n",
    "            \n",
    "        print(\"All models and data loaded successfully!\")\n",
    "        \n",
    "        return {\n",
    "            'train_df': train_df,\n",
    "            'test_df': test_df,\n",
    "            'encoders': encoders,\n",
    "            'eng_data': eng_data,\n",
    "            'model': model,\n",
    "            'feature_names': train_df.columns\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading models and data: {str(e)}\")\n",
    "        print(\"Error details:\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "    \n",
    "    # Load all models and data once\n",
    "print(\"Initializing prediction system...\")\n",
    "MODELS_AND_DATA = load_models_and_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def predict_movie_revenue(budget, runtime, genres, language, production_company, \n",
    "                         production_country, actor1, actor2, crew_director, \n",
    "                         crew_writer, crew_producer):\n",
    "    try:\n",
    "        # Load all transformation data\n",
    "        with open('/workspace/Film_Hit_prediction/jupyter_notebooks/outputs/engineered/full_transformation_data.pkl', 'rb') as f:\n",
    "            transform_data = pickle.load(f)\n",
    "            \n",
    "        # Load crew-specific data\n",
    "        with open('/workspace/Film_Hit_prediction/jupyter_notebooks/outputs/engineered/top_revenue_actors.pkl', 'rb') as f:\n",
    "            actor_data = pickle.load(f)\n",
    "        with open('/workspace/Film_Hit_prediction/jupyter_notebooks/outputs/engineered/top_revenue_directors.pkl', 'rb') as f:\n",
    "            director_data = pickle.load(f)\n",
    "        with open('/workspace/Film_Hit_prediction/jupyter_notebooks/outputs/engineered/top_revenue_writers.pkl', 'rb') as f:\n",
    "            writer_data = pickle.load(f)\n",
    "        with open('/workspace/Film_Hit_prediction/jupyter_notebooks/outputs/engineered/top_revenue_producers.pkl', 'rb') as f:\n",
    "            producer_data = pickle.load(f)\n",
    "\n",
    "        # Create initial features dictionary\n",
    "        features = {}\n",
    "        \n",
    "        # 1. Process budget features\n",
    "        budget_per_minute = budget / runtime if runtime > 0 else 0\n",
    "        features['budget'] = budget\n",
    "        features['budget_per_minute'] = budget_per_minute\n",
    "        \n",
    "        # 2. Process genres\n",
    "        for genre in transform_data['genre_columns']:\n",
    "            features[genre] = 1 if genre in genres else 0\n",
    "            \n",
    "        # 3. Process actors\n",
    "        actor_cols = actor_data['columns']\n",
    "        actor_metrics = actor_data['metrics']\n",
    "        \n",
    "        for col in actor_cols:\n",
    "            actor_name = col.replace('cast_', '')\n",
    "            is_in_movie = actor_name in [actor1, actor2]\n",
    "            features[col] = 1 if is_in_movie else 0\n",
    "            if is_in_movie and actor_name in actor_metrics:\n",
    "                features[f\"{col}_pop_weight\"] = actor_metrics[actor_name]['avg_popularity']\n",
    "            else:\n",
    "                features[f\"{col}_pop_weight\"] = 0\n",
    "                \n",
    "        # 4. Process director\n",
    "        director_cols = director_data['columns']\n",
    "        for col in director_cols:\n",
    "            director_name = col.replace('crew_director_', '')\n",
    "            features[col] = 1 if director_name == crew_director else 0\n",
    "            \n",
    "        # 5. Similar processing for writers and producers\n",
    "        # ... (similar to directors)\n",
    "        \n",
    "        # 6. Create DataFrame\n",
    "        pred_df = pd.DataFrame([features])\n",
    "        \n",
    "        # 7. Scale numerical features\n",
    "        numeric_cols = transform_data['numeric_cols']\n",
    "        pred_df[numeric_cols] = transform_data['feature_scaler'].transform(pred_df[numeric_cols])\n",
    "        \n",
    "        # 8. Ensure all features are present in correct order\n",
    "        for feature in transform_data['all_features']:\n",
    "            if feature not in pred_df.columns:\n",
    "                pred_df[feature] = 0\n",
    "                \n",
    "        pred_df = pred_df[transform_data['all_features']]\n",
    "        \n",
    "        # 9. Make prediction\n",
    "        raw_prediction = model.predict(pred_df)[0]\n",
    "        \n",
    "        # 10. Unscale prediction\n",
    "        predicted_revenue = (raw_prediction * transform_data['train_stats']['revenue_std'] + \n",
    "                           transform_data['train_stats']['revenue_mean'])\n",
    "        predicted_revenue = max(0, predicted_revenue)\n",
    "        \n",
    "        return {\n",
    "            'revenue': predicted_revenue,\n",
    "            'profit': predicted_revenue - budget,\n",
    "            'roi': ((predicted_revenue - budget) / budget * 100) if budget > 0 else 0,\n",
    "            'is_profitable': predicted_revenue > budget\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in prediction: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "def predict_movie_revenue(budget, runtime, genres, language, production_company, \n",
    "                         production_country, actor1, actor2, crew_director, \n",
    "                         crew_writer, crew_producer):\n",
    "    try:\n",
    "        # Load all transformation data\n",
    "        with open('/workspace/Film_Hit_prediction/jupyter_notebooks/outputs/engineered/full_transformation_data.pkl', 'rb') as f:\n",
    "            transform_data = pickle.load(f)\n",
    "            \n",
    "        # Load crew-specific data\n",
    "        with open('/workspace/Film_Hit_prediction/jupyter_notebooks/outputs/engineered/top_revenue_actors.pkl', 'rb') as f:\n",
    "            actor_data = pickle.load(f)\n",
    "        with open('/workspace/Film_Hit_prediction/jupyter_notebooks/outputs/engineered/top_revenue_directors.pkl', 'rb') as f:\n",
    "            director_data = pickle.load(f)\n",
    "        with open('/workspace/Film_Hit_prediction/jupyter_notebooks/outputs/engineered/top_revenue_writers.pkl', 'rb') as f:\n",
    "            writer_data = pickle.load(f)\n",
    "        with open('/workspace/Film_Hit_prediction/jupyter_notebooks/outputs/engineered/top_producers.pkl', 'rb') as f:\n",
    "            producer_data = pickle.load(f) \n",
    "\n",
    "        # Create initial features dictionary\n",
    "        features = {}\n",
    "        \n",
    "        # Initialize features dictionary with ALL possible features\n",
    "        features = {col: 0 for col in feature_names if col != 'revenue'}\n",
    "        \n",
    "        # Scale the numeric inputs using training data statistics\n",
    "        std_budget = (budget - train_df['budget'].mean()) / train_df['budget'].std()\n",
    "        budget_per_minute = budget / runtime if runtime > 0 else 0\n",
    "        std_budget_per_minute = (budget_per_minute - train_df['budget_per_minute'].mean()) / train_df['budget_per_minute'].std()\n",
    "        \n",
    "        print(\"\\nDEBUG - Scaled Inputs:\")\n",
    "        print(f\"Original budget: ${budget:,.2f}\")\n",
    "        print(f\"Scaled budget: {std_budget:.4f}\")\n",
    "        print(f\"Original budget_per_minute: ${budget_per_minute:,.2f}\")\n",
    "        print(f\"Scaled budget_per_minute: {std_budget_per_minute:.4f}\")\n",
    "        \n",
    "        # Set scaled numeric features\n",
    "        features['budget'] = std_budget\n",
    "        features['popularity'] = 0  # Default for new movies\n",
    "        features['runtime'] = runtime\n",
    "        features['budget_per_minute'] = std_budget_per_minute\n",
    "\n",
    "        # Set categorical features\n",
    "        print(\"\\nDEBUG - Setting categorical features:\")\n",
    "        if f'cast_{actor1}' in features:\n",
    "            features[f'cast_{actor1}'] = 1\n",
    "            print(f\"Set {actor1}\")\n",
    "        if f'cast_{actor2}' in features:\n",
    "            features[f'cast_{actor2}'] = 1\n",
    "            print(f\"Set {actor2}\")\n",
    "        \n",
    "        if f'crew_director_{crew_director}' in features:\n",
    "            features[f'crew_director_{crew_director}'] = 1\n",
    "            print(f\"Set director {crew_director}\")\n",
    "        \n",
    "        if f'crew_writer_{crew_writer}' in features:\n",
    "            features[f'crew_writer_{crew_writer}'] = 1\n",
    "            print(f\"Set writer {crew_writer}\")\n",
    "        \n",
    "        if f'crew_producer_{crew_producer}' in features:\n",
    "            features[f'crew_producer_{crew_producer}'] = 1\n",
    "            print(f\"Set producer {crew_producer}\")\n",
    "            \n",
    "        if f'company_{production_company}' in features:\n",
    "            features[f'company_{production_company}'] = 1\n",
    "            print(f\"Set company {production_company}\")\n",
    "\n",
    "        # Set genre features\n",
    "        print(\"\\nDEBUG - Setting genres:\")\n",
    "        for genre in genres:\n",
    "            if genre in features:\n",
    "                features[genre] = 1\n",
    "                print(f\"Set genre {genre}\")\n",
    "\n",
    "        # Create DataFrame and ensure all features are present\n",
    "        pred_df = pd.DataFrame([features])\n",
    "        \n",
    "        # Explicitly reindex to match model's expected features\n",
    "        expected_features = [col for col in feature_names if col != 'revenue']\n",
    "        pred_df = pred_df.reindex(columns=expected_features, fill_value=0)\n",
    "        \n",
    "        print(f\"\\nDEBUG - Features shape after reindexing: {pred_df.shape}\")\n",
    "        print(f\"Number of non-zero features: {(pred_df != 0).sum().sum()}\")\n",
    "        \n",
    "        # Get prediction\n",
    "        raw_prediction = model.predict(pred_df)[0]\n",
    "        print(f\"Raw scaled prediction: {raw_prediction}\")\n",
    "        \n",
    "        # Unscale the prediction using training data statistics\n",
    "        predicted_revenue = (raw_prediction * train_df['revenue'].std()) + train_df['revenue'].mean()\n",
    "        predicted_revenue = max(0, predicted_revenue)\n",
    "        \n",
    "        return {\n",
    "            'revenue': predicted_revenue,\n",
    "            'profit': predicted_revenue - budget,\n",
    "            'roi': ((predicted_revenue - budget) / budget * 100) if budget > 0 else 0,\n",
    "            'is_profitable': predicted_revenue > budget\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in prediction: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    test_movie = {\n",
    "        'budget': 150000000,\n",
    "        'runtime': 120,\n",
    "        'genres': ['Action'],\n",
    "        'language': 'en',\n",
    "        'production_company': 'Marvel Studios',\n",
    "        'production_country': 'US',\n",
    "        'actor1': 'Chris Hemsworth',\n",
    "        'actor2': 'Robert Downey Jr.',\n",
    "        'crew_director': 'Joss Whedon',\n",
    "        'crew_writer': 'Joss Whedon',\n",
    "        'crew_producer': 'Kevin Feige'\n",
    "    }\n",
    "    \n",
    "    result = predict_movie_revenue(**test_movie)\n",
    "    \n",
    "    if result is not None:\n",
    "        print(\"\\nPredicted Results:\")\n",
    "        print(f\"\\nPredicted Revenue: ${result['revenue']:,.2f}\")\n",
    "        print(f\"Predicted Profit/Loss: ${result['profit']:,.2f}\")\n",
    "        print(f\"ROI: {result['roi']:.1f}%\")\n",
    "        print(f\"Is Profitable: {'Yes' if result['is_profitable'] else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "model_path = \"/workspace/Film_Hit_prediction/jupyter_notebooks/outputs/models/\"  \n",
    "model = joblib.load(model_path + 'film_revenue_model.joblib')\n",
    "\n",
    "print(\"Model loaded:\", type(model))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions on test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load test and train data\n",
    "with open('/workspace/Film_Hit_prediction/jupyter_notebooks/outputs/engineered/y_test.pkl', 'rb') as f:\n",
    "    y_test = pickle.load(f)\n",
    "    \n",
    "with open('/workspace/Film_Hit_prediction/jupyter_notebooks/outputs/engineered/y_train.pkl', 'rb') as f:\n",
    "    y_train = pickle.load(f)\n",
    "\n",
    "# Load X_test if not already loaded\n",
    "with open('/workspace/Film_Hit_prediction/jupyter_notebooks/outputs/engineered/X_test.pkl', 'rb') as f:\n",
    "    X_test = pickle.load(f)\n",
    "\n",
    "# Get predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\nModel Evaluation Metrics:\")\n",
    "print(f'Root Mean Squared Error: ${rmse:,.2f}')\n",
    "print(f'Mean Absolute Error: ${mae:,.2f}')\n",
    "print(f'R² Score: {r2:.4f}')\n",
    "\n",
    "# Calculate percentage error\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "print(f'Mean Absolute Percentage Error: {mape:.2f}%')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of predictions vs actual\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Revenue')\n",
    "plt.ylabel('Predicted Revenue')\n",
    "plt.title('Predicted vs Actual Revenue')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Residual plot\n",
    "residuals = y_test - y_pred\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_pred, residuals, alpha=0.5)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted Revenue')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Distribution of residuals\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(residuals, kde=True)\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Residuals')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Additional analysis\n",
    "print(\"\\nResiduals Analysis:\")\n",
    "print(f\"Mean of residuals: ${residuals.mean():,.2f}\")\n",
    "print(f\"Standard deviation of residuals: ${residuals.std():,.2f}\")\n",
    "print(f\"Skewness of residuals: {residuals.skew():.2f}\")\n",
    "\n",
    "# Sample predictions\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Actual Revenue': y_test,\n",
    "    'Predicted Revenue': y_pred,\n",
    "    'Absolute Error': np.abs(y_test - y_pred),\n",
    "    'Percentage Error': np.abs((y_test - y_pred) / y_test) * 100\n",
    "})\n",
    "\n",
    "print(\"\\nSample Predictions (first 5 movies):\")\n",
    "print(comparison_df.head().to_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
