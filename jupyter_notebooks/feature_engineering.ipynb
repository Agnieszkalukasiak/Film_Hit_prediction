{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Objectives\n",
    "\n",
    "*   Engineer features for Regression model\n",
    "\n",
    "\n",
    "## Inputs\n",
    "\n",
    "* inputs/datasets/cleaned/test_df_cleaned.pkl\n",
    "* inputs/datasets/cleaned/train_df_cleaned.pkl\n",
    "\n",
    "## Outputs\n",
    "\n",
    "* generate a list with variables to engineer\n",
    "\n",
    "## Conclusions\n",
    "\n",
    "* Feature Engineering Transformers\n",
    "* \n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to change the working directory from its current folder to its parent folder\n",
    "* We access the current directory with os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/Film_Hit_prediction'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to make the parent of the current directory the new current directory.\n",
    "* os.path.dirname() gets the parent directory\n",
    "* os.chir() defines the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You set a new current directory\n"
     ]
    }
   ],
   "source": [
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"You set a new current directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Cleaned Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 genres original_language  \\\n",
      "2851  [{\"id\": 35, \"name\": \"Comedy\"}, {\"id\": 10751, \"...                en   \n",
      "3262  [{\"id\": 35, \"name\": \"Comedy\"}, {\"id\": 18, \"nam...                en   \n",
      "4579  [{\"id\": 12, \"name\": \"Adventure\"}, {\"id\": 35, \"...                en   \n",
      "\n",
      "        budget   revenue  runtime  \\\n",
      "2851  12000000  17292381     86.0   \n",
      "3262         0  25288872     93.0   \n",
      "4579    400000   5028948     91.0   \n",
      "\n",
      "                                   production_companies  \\\n",
      "2851  [{\"name\": \"Walt Disney Pictures\", \"id\": 2}, {\"...   \n",
      "3262   [{\"name\": \"Fox Searchlight Pictures\", \"id\": 43}]   \n",
      "4579  [{\"name\": \"Python (Monty) Pictures Limited\", \"...   \n",
      "\n",
      "                                   production_countries  \\\n",
      "2851  [{\"iso_3166_1\": \"US\", \"name\": \"United States o...   \n",
      "3262  [{\"iso_3166_1\": \"US\", \"name\": \"United States o...   \n",
      "4579   [{\"iso_3166_1\": \"GB\", \"name\": \"United Kingdom\"}]   \n",
      "\n",
      "                                                   cast  \\\n",
      "2851  [{\"cast_id\": 1, \"character\": \"Max Keeble\", \"cr...   \n",
      "3262  [{\"cast_id\": 3, \"character\": \"Eva\", \"credit_id...   \n",
      "4579  [{\"cast_id\": 11, \"character\": \"King Arthur / V...   \n",
      "\n",
      "                                                   crew  popularity  \n",
      "2851  [{\"credit_id\": \"52fe456a9251416c91031963\", \"de...    1.081822  \n",
      "3262  [{\"credit_id\": \"52fe4d60c3a368484e1e5c23\", \"de...   14.969093  \n",
      "4579  [{\"credit_id\": \"52fe4272c3a36847f801f66f\", \"de...   64.782984  \n",
      "Shape of the dataframe: (3841, 10)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Correct path relative to the current directory\n",
    "Train_set_path = \"/workspace/Film_Hit_prediction/jupyter_notebooks/outputs/cleaned/train_df_cleaned.pkl\"\n",
    "\n",
    "try:\n",
    "    TrainSet = pd.read_pickle(Train_set_path)\n",
    "    print(TrainSet.head(3))\n",
    "    print(\"Shape of the dataframe:\", TrainSet.shape)\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found at path: {Train_set_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 genres original_language  \\\n",
      "596   [{\"id\": 28, \"name\": \"Action\"}, {\"id\": 12, \"nam...                en   \n",
      "4507  [{\"id\": 35, \"name\": \"Comedy\"}, {\"id\": 10402, \"...                en   \n",
      "3049  [{\"id\": 35, \"name\": \"Comedy\"}, {\"id\": 10751, \"...                en   \n",
      "\n",
      "        budget   revenue  runtime  \\\n",
      "596   70000000  33561137     97.0   \n",
      "4507    560000  12299668     88.0   \n",
      "3049         0         0     89.0   \n",
      "\n",
      "                                   production_companies  \\\n",
      "596   [{\"name\": \"Columbia Pictures Corporation\", \"id...   \n",
      "4507  [{\"name\": \"Proscenium Films\", \"id\": 413}, {\"na...   \n",
      "3049                                                 []   \n",
      "\n",
      "                                   production_countries  \\\n",
      "596   [{\"iso_3166_1\": \"US\", \"name\": \"United States o...   \n",
      "4507   [{\"iso_3166_1\": \"GB\", \"name\": \"United Kingdom\"}]   \n",
      "3049  [{\"iso_3166_1\": \"US\", \"name\": \"United States o...   \n",
      "\n",
      "                                                   cast  \\\n",
      "596   [{\"cast_id\": 1, \"character\": \"Kelly Robinson\",...   \n",
      "4507  [{\"cast_id\": 10, \"character\": \"himself\", \"cred...   \n",
      "3049  [{\"cast_id\": 18, \"character\": \"Julie Corky\", \"...   \n",
      "\n",
      "                                                   crew  popularity  \n",
      "596   [{\"credit_id\": \"52fe44a8c3a36847f80a280d\", \"de...   13.267631  \n",
      "4507  [{\"credit_id\": \"52fe426dc3a36847f801dc85\", \"de...   10.730056  \n",
      "3049  [{\"credit_id\": \"52fe4542c3a36847f80c4103\", \"de...    5.842299  \n",
      "Shape of the dataframe: (961, 10)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Correct path relative to the current directory\n",
    "Test_set_path = \"/workspace/Film_Hit_prediction/jupyter_notebooks/outputs/cleaned/test_df_cleaned.pkl\"\n",
    "\n",
    "try:\n",
    "    TestSet = pd.read_pickle(Test_set_path)\n",
    "    print(TestSet.head(3))\n",
    "    print(\"Shape of the dataframe:\", TestSet.shape)\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found at path: {Test_set_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate potential transformations to be made\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Overview:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1000 entries, 2638 to 1509\n",
      "Data columns (total 10 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   genres                1000 non-null   object \n",
      " 1   original_language     1000 non-null   object \n",
      " 2   budget                1000 non-null   int64  \n",
      " 3   revenue               1000 non-null   int64  \n",
      " 4   runtime               1000 non-null   float64\n",
      " 5   production_companies  1000 non-null   object \n",
      " 6   production_countries  1000 non-null   object \n",
      " 7   cast                  1000 non-null   object \n",
      " 8   crew                  1000 non-null   object \n",
      " 9   popularity            1000 non-null   float64\n",
      "dtypes: float64(2), int64(2), object(6)\n",
      "memory usage: 85.9+ KB\n",
      "None\n",
      "\n",
      "Summary Statistics:\n",
      "             budget       revenue      runtime   popularity\n",
      "count  1.000000e+03  1.000000e+03  1000.000000  1000.000000\n",
      "mean   2.876460e+07  7.902145e+07   107.001000    21.825199\n",
      "std    4.142592e+07  1.620610e+08    23.309232    39.313921\n",
      "min    0.000000e+00  0.000000e+00     0.000000     0.001117\n",
      "25%    6.437500e+05  0.000000e+00    94.000000     5.010213\n",
      "50%    1.360000e+07  1.803848e+07   104.000000    12.437092\n",
      "75%    3.825000e+07  9.028572e+07   117.000000    27.921399\n",
      "max    2.800000e+08  1.519558e+09   338.000000   875.581305\n",
      "\n",
      "Missing Values:\n",
      "genres                  0\n",
      "original_language       0\n",
      "budget                  0\n",
      "revenue                 0\n",
      "runtime                 0\n",
      "production_companies    0\n",
      "production_countries    0\n",
      "cast                    0\n",
      "crew                    0\n",
      "popularity              0\n",
      "dtype: int64\n",
      "\n",
      "Top 5 Rows:\n",
      "                                                 genres original_language  \\\n",
      "2638  [{\"id\": 18, \"name\": \"Drama\"}, {\"id\": 878, \"nam...                de   \n",
      "3092  [{\"id\": 35, \"name\": \"Comedy\"}, {\"id\": 18, \"nam...                en   \n",
      "4138                     [{\"id\": 27, \"name\": \"Horror\"}]                en   \n",
      "2079  [{\"id\": 10749, \"name\": \"Romance\"}, {\"id\": 18, ...                en   \n",
      "1847  [{\"id\": 18, \"name\": \"Drama\"}, {\"id\": 80, \"name...                en   \n",
      "\n",
      "        budget   revenue  runtime  \\\n",
      "2638  92620000    650422    153.0   \n",
      "3092  10000000   7022728     96.0   \n",
      "4138    100000     95000     86.0   \n",
      "2079  21000000  10011050    117.0   \n",
      "1847  25000000  46836394    145.0   \n",
      "\n",
      "                                   production_companies  \\\n",
      "2638  [{\"name\": \"Paramount Pictures\", \"id\": 4}, {\"na...   \n",
      "3092  [{\"name\": \"Echo Lake Productions\", \"id\": 2147}...   \n",
      "4138  [{\"name\": \"B\\u00f3rd Scann\\u00e1n na h\\u00c9ir...   \n",
      "2079  [{\"name\": \"Punch Productions\", \"id\": 2154}, {\"...   \n",
      "1847            [{\"name\": \"Winkler Films\", \"id\": 8880}]   \n",
      "\n",
      "                                   production_countries  \\\n",
      "2638          [{\"iso_3166_1\": \"DE\", \"name\": \"Germany\"}]   \n",
      "3092  [{\"iso_3166_1\": \"US\", \"name\": \"United States o...   \n",
      "4138          [{\"iso_3166_1\": \"IE\", \"name\": \"Ireland\"}]   \n",
      "2079  [{\"iso_3166_1\": \"US\", \"name\": \"United States o...   \n",
      "1847  [{\"iso_3166_1\": \"US\", \"name\": \"United States o...   \n",
      "\n",
      "                                                   cast  \\\n",
      "2638  [{\"cast_id\": 10, \"character\": \"Maria\", \"credit...   \n",
      "3092  [{\"cast_id\": 4, \"character\": \"Steve Jones\", \"c...   \n",
      "4138  [{\"cast_id\": 2, \"character\": \"Stitches\", \"cred...   \n",
      "2079  [{\"cast_id\": 3, \"character\": \"Joe Nast\", \"cred...   \n",
      "1847  [{\"cast_id\": 16, \"character\": \"James Conway\", ...   \n",
      "\n",
      "                                                   crew  popularity  \n",
      "2638  [{\"credit_id\": \"52fe420fc3a36847f8000c55\", \"de...   32.351527  \n",
      "3092  [{\"credit_id\": \"52fe45cdc3a36847f80db247\", \"de...   14.979375  \n",
      "4138  [{\"credit_id\": \"551becafc3a368766c0032c6\", \"de...    7.447714  \n",
      "2079  [{\"credit_id\": \"555eec5c9251417e4f002317\", \"de...    8.982335  \n",
      "1847  [{\"credit_id\": \"52fe4274c3a36847f801fcc3\", \"de...   63.654244  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample a subset of the dataset (e.g., 1000 rows or 10%)\n",
    "sampled_df = TrainSet.sample(n=min(1000, len(TrainSet)), random_state=42)\n",
    "\n",
    "# Basic profiling\n",
    "print(\"Dataset Overview:\")\n",
    "print(sampled_df.info())\n",
    "\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(sampled_df.describe())\n",
    "\n",
    "print(\"\\nMissing Values:\")\n",
    "print(sampled_df.isnull().sum())\n",
    "\n",
    "print(\"\\nTop 5 Rows:\")\n",
    "print(sampled_df.head())\n",
    "\n",
    "# Save the sampled data if needed for further exploration\n",
    "sampled_df.to_csv('sampled_dataset.csv', index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation and PPS Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We don’t expect changes compared to the data cleaning notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for top actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "def top_revenue_actors(TrainSet, TestSet, n_actors=30):\n",
    "\n",
    "    # Create copies and get cast columns once\n",
    "    train_copy, test_copy = TrainSet.copy(), TestSet.copy()\n",
    "    cast_cols = [col for col in TrainSet.columns if col.startswith('cast_')]\n",
    "\n",
    "    # Calculate multiple metrics for each actor\n",
    "    actor_metrics = {}\n",
    "    for col in cast_cols:\n",
    "        actor_name = col.replace('cast_', '')\n",
    "        movies_with_actor = TrainSet[TrainSet[col] == 1]\n",
    "        movies_count = len(movies_with_actor)\n",
    "\n",
    "        metrics = {\n",
    "            'movies_count': movies_count,\n",
    "            'total_revenue': movies_with_actor['revenue'].sum(),\n",
    "            'avg_revenue': movies_with_actor['revenue'].mean(),\n",
    "            'revenue_consistency': movies_with_actor['revenue'].std(),\n",
    "            'hit_rate': (movies_with_actor['revenue'] > movies_with_actor['revenue'].mean()).mean(),\n",
    "            'avg_popularity': movies_with_actor['popularity'].mean(),\n",
    "            'popularity_consistency': movies_with_actor['popularity'].std(),\n",
    "            'revenue_popularity_correlation': movies_with_actor[['revenue', 'popularity']].corr().iloc[0,1]\n",
    "        }          \n",
    "        actor_metrics[actor_name] = metrics\n",
    "\n",
    "         # Calculate composite scores\n",
    "    for actor, metrics in actor_metrics.items():\n",
    "        # Calculate normalized metrics\n",
    "        revenue_norm = metrics['total_revenue'] / max(m['total_revenue'] for m in actor_metrics.values())\n",
    "        avg_norm = metrics['avg_revenue'] / max(m['avg_revenue'] for m in actor_metrics.values())\n",
    "        consistency_norm = 1 - (metrics['revenue_consistency'] / max(m['revenue_consistency'] for m in actor_metrics.values()))\n",
    "        popularity_norm = metrics['avg_popularity'] / max(m['avg_popularity'] for m in actor_metrics.values())\n",
    "        correlation_norm = abs(metrics['revenue_popularity_correlation'])\n",
    "        \n",
    "        # Composite score with popularity factors\n",
    "        metrics['composite_score'] = (\n",
    "            0.3 * revenue_norm +           # Total revenue importance\n",
    "            0.2 * avg_norm +               # Average revenue importance\n",
    "            0.2 * consistency_norm +       # Revenue consistency importance\n",
    "            0.2 * popularity_norm +        # Popularity importance\n",
    "            0.1 * correlation_norm         # Revenue-popularity correlation importance\n",
    "        )\n",
    "\n",
    "    # Get top actors based on composite score\n",
    "    top_actors = sorted(actor_metrics.items(), key=lambda x: x[1]['composite_score'], reverse=True)[:n_actors]\n",
    "    top_actor_cols = [f\"cast_{actor}\" for actor, _ in top_actors]\n",
    "\n",
    "    print(f\"Number of columns after adding top actor features: {len(train_copy.columns) + 2*n_actors}\") \n",
    "\n",
    "    # Process both DataFrames\n",
    "    processed_dfs = []\n",
    "    for df in [train_copy, test_copy]:\n",
    "        # Keep original non-cast columns + top actor columns\n",
    "        keep_cols = [col for col in df.columns if not col.startswith('cast_')] + top_actor_cols\n",
    "        processed = df[keep_cols].copy()\n",
    "        \n",
    "        # Add popularity weighted features for top actors\n",
    "        for actor_col in top_actor_cols:\n",
    "            if actor_col in df.columns:\n",
    "                actor_name = actor_col.replace('cast_', '')\n",
    "                processed[f\"{actor_col}_pop_weight\"] = (\n",
    "                    df[actor_col] * actor_metrics[actor_name]['avg_popularity']\n",
    "                )\n",
    "            else:\n",
    "                processed[actor_col] = 0\n",
    "                processed[f\"{actor_col}_pop_weight\"] = 0\n",
    "        \n",
    "        # Add other actor count\n",
    "        other_cast_cols = [col for col in cast_cols if col not in top_actor_cols]\n",
    "        processed['other_actor_count'] = df[other_cast_cols].sum(axis=1)\n",
    "        processed_dfs.append(processed)\n",
    "\n",
    "    # Save top actors data\n",
    "    with open('/workspace/Film_Hit_prediction/jupyter_notebooks/outputs/engineered/top_revenue_actors.pkl', 'wb') as f:\n",
    "        pickle.dump({'columns': top_actor_cols, 'metrics': actor_metrics}, f)\n",
    "\n",
    "    return processed_dfs[0], processed_dfs[1]\n",
    "    print(f\"Final shape - TrainSet_processed: {processed_dfs[0].shape}, TestSet_processed: {processed_dfs[1].shape}\")\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns after adding top actor features: 70\n"
     ]
    }
   ],
   "source": [
    "TrainSet_processed, TestSet_processed = top_revenue_actors(TrainSet, TestSet, n_actors=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for top directors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "def top_revenue_directors(TrainSet, TestSet, n_directors=20):\n",
    "    # Create copies of input data to avoid modifications\n",
    "    train_copy = TrainSet.copy()\n",
    "    test_copy = TestSet.copy()\n",
    "\n",
    "    # Find top revenue-generating directors\n",
    "    director_cols = [col for col in TrainSet.columns if col.startswith('crew_Director_')]\n",
    "\n",
    "    print(f\"Number of director columns found: {len(director_cols)}\")\n",
    "    print(\"First few director columns:\", director_cols[:5])\n",
    "    \n",
    "    # Calculate multiple metrics for each director\n",
    "    director_metrics = {}\n",
    "    for col in director_cols:\n",
    "        director_name = col.replace('crew_Director_', '')\n",
    "        movies_with_director = TrainSet[TrainSet[col] == 1]\n",
    "        movies_count = TrainSet[col].sum()\n",
    "    \n",
    "        metrics = {\n",
    "            'movies_count': movies_count,\n",
    "            'total_revenue': movies_with_director['revenue'].sum(),\n",
    "            'avg_revenue': movies_with_director['revenue'].mean(),\n",
    "            'revenue_consistency': movies_with_director['revenue'].std(),\n",
    "            'hit_rate': (movies_with_director['revenue'] > movies_with_director['revenue'].mean()).mean(),\n",
    "            'avg_popularity': movies_with_director['popularity'].mean(),\n",
    "            'popularity_consistency': movies_with_director['popularity'].std(),\n",
    "            'revenue_popularity_correlation': movies_with_director[['revenue', 'popularity']].corr().iloc[0,1]\n",
    "        }\n",
    "        director_metrics[director_name] = metrics\n",
    "\n",
    "    # Calculate composite scores\n",
    "    for director, metrics in director_metrics.items():\n",
    "        # Calculate normalized metrics\n",
    "        revenue_norm = metrics['total_revenue'] / max(m['total_revenue'] for m in director_metrics.values())\n",
    "        avg_norm = metrics['avg_revenue'] / max(m['avg_revenue'] for m in director_metrics.values())\n",
    "        consistency_norm = 1 - (metrics['revenue_consistency'] / max(m['revenue_consistency'] for m in director_metrics.values()))\n",
    "        popularity_norm = metrics['avg_popularity'] / max(m['avg_popularity'] for m in director_metrics.values())\n",
    "        correlation_norm = abs(metrics['revenue_popularity_correlation'])\n",
    "        \n",
    "        metrics['composite_score'] = (\n",
    "            0.3 * revenue_norm +           # Total revenue importance\n",
    "            0.2 * avg_norm +               # Average revenue importance\n",
    "            0.2 * consistency_norm +       # Revenue consistency importance\n",
    "            0.2 * popularity_norm +        # Popularity importance\n",
    "            0.1 * correlation_norm         # Revenue-popularity correlation importance\n",
    "        )\n",
    "\n",
    "    # Get top directors based on composite score\n",
    "    top_directors = sorted(director_metrics.items(), key=lambda x: x[1]['composite_score'], reverse=True)[:n_directors]\n",
    "    top_director_cols = [f\"crew_Director_{director}\" for director, _ in top_directors]\n",
    "\n",
    "    # Process both DataFrames\n",
    "    processed_dfs = []\n",
    "    for df in [train_copy, test_copy]:\n",
    "        # Keep original non-director columns + top director columns\n",
    "        keep_cols = [col for col in df.columns if not col.startswith('crew_Director_')] + top_director_cols\n",
    "        processed = df[keep_cols].copy()\n",
    "        \n",
    "        # Add popularity weighted features for top directors\n",
    "        for director_col in top_director_cols:\n",
    "            if director_col in df.columns:\n",
    "                director_name = director_col.replace('crew_Director_', '')\n",
    "                processed[f\"{director_col}_pop_weight\"] = (\n",
    "                    df[director_col] * director_metrics[director_name]['avg_popularity']\n",
    "                )\n",
    "            else:\n",
    "                processed[director_col] = 0\n",
    "                processed[f\"{director_col}_pop_weight\"] = 0\n",
    "        \n",
    "        # Add other director count\n",
    "        other_director_cols = [col for col in director_cols if col not in top_director_cols]\n",
    "        processed['other_director_count'] = df[other_director_cols].sum(axis=1)\n",
    "        processed_dfs.append(processed)\n",
    "\n",
    "    # Save top directors data\n",
    "    with open('/workspace/Film_Hit_prediction/jupyter_notebooks/outputs/engineered/top_revenue_directors.pkl', 'wb') as f:\n",
    "        pickle.dump({'columns': top_director_cols, 'metrics': director_metrics}, f)\n",
    "\n",
    "    return processed_dfs[0], processed_dfs[1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting director feature engineering...\n",
      "Number of director columns found: 0\n",
      "First few director columns: []\n",
      "\n",
      "Feature Engineering Summary:\n",
      "--------------------\n",
      "Top directors analyzed by:\n",
      "- Total revenue\n",
      "- Average popularity\n",
      "- Revenue-popularity correlation\n",
      "\n",
      "Dataset Shapes:\n",
      "Processed train shape: (3841, 11)\n",
      "Processed test shape: (961, 11)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStarting director feature engineering...\")\n",
    "TrainSet_processed, TestSet_processed = top_revenue_directors(TrainSet, TestSet, n_directors=20)\n",
    "\n",
    "# See what the processed data looks like\n",
    "print(\"\\nFeature Engineering Summary:\")\n",
    "print(\"-\" * 20)\n",
    "print(\"Top directors analyzed by:\")\n",
    "print(\"- Total revenue\")\n",
    "print(\"- Average popularity\")\n",
    "print(\"- Revenue-popularity correlation\")\n",
    "print(\"\\nDataset Shapes:\")\n",
    "print(f\"Processed train shape: {TrainSet_processed.shape}\")\n",
    "print(f\"Processed test shape: {TestSet_processed.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for top writers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_revenue_writers(TrainSet, TestSet, n_writers=10):\n",
    "    # Create copies and get writer columns once\n",
    "    train_copy, test_copy = TrainSet.copy(), TestSet.copy()\n",
    "    writer_cols = [col for col in TrainSet.columns if col.startswith('crew_Writer_')]\n",
    "\n",
    "    print(\"\\nNumber of writer columns found:\", len(writer_cols))\n",
    "    print(\"First few writer columns:\", writer_cols[:5])\n",
    "\n",
    "    # Calculate writer metrics in one pass\n",
    "    writer_metrics = {}\n",
    "    for col in writer_cols:\n",
    "        writer_name = col.replace('crew_Writer_', '')\n",
    "        movies_with_writer = TrainSet[TrainSet[col] == 1]\n",
    "        movies_count = len(movies_with_writer)\n",
    "        \n",
    "        metrics = {\n",
    "            'movies_count': movies_count,\n",
    "            'total_revenue': movies_with_writer['revenue'].sum(),\n",
    "            'avg_revenue': movies_with_writer['revenue'].mean(),\n",
    "            'revenue_consistency': movies_with_writer['revenue'].std(),\n",
    "            'avg_popularity': movies_with_writer['popularity'].mean(),\n",
    "            'popularity_consistency': movies_with_writer['popularity'].std(),\n",
    "            'revenue_popularity_correlation': movies_with_writer[['revenue', 'popularity']].corr().iloc[0,1]\n",
    "        }\n",
    "        writer_metrics[writer_name] = metrics\n",
    "\n",
    "    # Print information about all writers\n",
    "    print(f\"\\nFound {len(writer_metrics)} writers\")\n",
    "    print(\"\\nAll writers and their metrics:\")\n",
    "    for writer, metrics in writer_metrics.items():\n",
    "        print(f\"{writer}: {metrics['movies_count']} movies, ${metrics['total_revenue']:,.2f} total revenue\")\n",
    "\n",
    "    if not writer_metrics:\n",
    "        print(\"No writers found.\")\n",
    "        return train_copy, test_copy\n",
    "\n",
    "    # Calculate composite scores\n",
    "    for writer, metrics in writer_metrics.items():\n",
    "        max_revenue = max(m['total_revenue'] for m in writer_metrics.values())\n",
    "        max_avg_revenue = max(m['avg_revenue'] for m in writer_metrics.values())\n",
    "        max_revenue_consistency = max(m['revenue_consistency'] for m in writer_metrics.values())\n",
    "        max_popularity = max(m['avg_popularity'] for m in writer_metrics.values())\n",
    "        \n",
    "        revenue_norm = metrics['total_revenue'] / max_revenue if max_revenue > 0 else 0\n",
    "        avg_norm = metrics['avg_revenue'] / max_avg_revenue if max_avg_revenue > 0 else 0\n",
    "        consistency_norm = 1 - (metrics['revenue_consistency'] / max_revenue_consistency) if max_revenue_consistency > 0 else 0\n",
    "        popularity_norm = metrics['avg_popularity'] / max_popularity if max_popularity > 0 else 0\n",
    "        correlation_norm = abs(metrics['revenue_popularity_correlation'])\n",
    "        \n",
    "        metrics['composite_score'] = (\n",
    "            0.3 * revenue_norm +           \n",
    "            0.2 * avg_norm +               \n",
    "            0.2 * consistency_norm +       \n",
    "            0.2 * popularity_norm +        \n",
    "            0.1 * correlation_norm         \n",
    "        )\n",
    "\n",
    "    # Get top writers based on composite score\n",
    "    top_writers = sorted(writer_metrics.items(), key=lambda x: x[1]['composite_score'], reverse=True)[:n_writers]\n",
    "    top_writer_cols = [f\"crew_Writer_{writer}\" for writer, _ in top_writers]\n",
    "\n",
    "    # Print top writers and their metrics\n",
    "    print(\"\\nTop writers by composite score:\")\n",
    "    for writer, metrics in top_writers:\n",
    "        print(f\"{writer}:\")\n",
    "        print(f\"  Movies: {metrics['movies_count']}\")\n",
    "        print(f\"  Total Revenue: ${metrics['total_revenue']:,.2f}\")\n",
    "        print(f\"  Avg Revenue: ${metrics['avg_revenue']:,.2f}\")\n",
    "        print(f\"  Composite Score: {metrics['composite_score']:.3f}\")\n",
    "\n",
    "    # Process both DataFrames\n",
    "    processed_dfs = []\n",
    "    for df in [train_copy, test_copy]:\n",
    "        # Keep original non-writer columns + top writer columns\n",
    "        keep_cols = [col for col in df.columns if not col.startswith('crew_Writer_')] + top_writer_cols\n",
    "        processed = df[keep_cols].copy()\n",
    "        \n",
    "        # Add popularity weighted features for top writers\n",
    "        for writer_col in top_writer_cols:\n",
    "            if writer_col in df.columns:\n",
    "                writer_name = writer_col.replace('crew_Writer_', '')\n",
    "                processed[f\"{writer_col}_pop_weight\"] = (\n",
    "                    df[writer_col] * writer_metrics[writer_name]['avg_popularity']\n",
    "                )\n",
    "            else:\n",
    "                processed[writer_col] = 0\n",
    "                processed[f\"{writer_col}_pop_weight\"] = 0\n",
    "        \n",
    "        # Add other writer count\n",
    "        other_writer_cols = [col for col in writer_cols if col not in top_writer_cols]\n",
    "        processed['other_writer_count'] = df[other_writer_cols].sum(axis=1)\n",
    "        processed_dfs.append(processed)\n",
    "\n",
    "    # Save top writers data\n",
    "    with open('/workspace/Film_Hit_prediction/jupyter_notebooks/outputs/engineered/top_revenue_writers.pkl', 'wb') as f:\n",
    "        pickle.dump({'columns': top_writer_cols, 'metrics': writer_metrics}, f)\n",
    "\n",
    "    return processed_dfs[0], processed_dfs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting writer feature engineering...\n",
      "\n",
      "Number of writer columns found: 0\n",
      "First few writer columns: []\n",
      "\n",
      "Found 0 writers\n",
      "\n",
      "All writers and their metrics:\n",
      "No writers found.\n",
      "\n",
      "Feature Engineering Summary:\n",
      "-----\n",
      "Top writers analyzed by:\n",
      "- Total revenue\n",
      "- Average popularity\n",
      "- Revenue-popularity correlation\n",
      "\n",
      "Dataset Shapes:\n",
      "Processed train shape: (3841, 10)\n",
      "Processed test shape: (961, 10)\n"
     ]
    }
   ],
   "source": [
    "# Call the writer function\n",
    "print(\"\\nStarting writer feature engineering...\")\n",
    "TrainSet_processed, TestSet_processed = top_revenue_writers(TrainSet, TestSet, n_writers=10)\n",
    "\n",
    "## See what the processed data looks like\n",
    "print(\"\\nFeature Engineering Summary:\")\n",
    "print(\"-\" * 5)\n",
    "print(\"Top writers analyzed by:\")\n",
    "print(\"- Total revenue\")\n",
    "print(\"- Average popularity\")\n",
    "print(\"- Revenue-popularity correlation\")\n",
    "print(\"\\nDataset Shapes:\")\n",
    "print(f\"Processed train shape: {TrainSet_processed.shape}\")\n",
    "print(f\"Processed test shape: {TestSet_processed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for top producers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    " \n",
    "def top_revenue_producers(TrainSet, TestSet, n_producers=20, min_movies=10):\n",
    "\n",
    "    # Create copies and get producer columns once\n",
    "    train_copy, test_copy = TrainSet.copy(), TestSet.copy()\n",
    "    producer_cols = [col for col in TrainSet.columns if col.startswith('crew_Producer_')]\n",
    "\n",
    "    print(\"\\nNumber of producer columns found:\", len(producer_cols))\n",
    "    print(\"First few producer columns:\", producer_cols[:5])\n",
    "\n",
    "    \n",
    "\n",
    "    # Calculate producer metrics in one pass\n",
    "    producer_metrics = {}\n",
    "    for col in producer_cols:\n",
    "        producer_name = col.replace('crew_Producer_', '')\n",
    "        movies_with_producer = TrainSet[TrainSet[col] == 1]\n",
    "        movies_count = len(movies_with_producer)\n",
    "        \n",
    "        metrics = {\n",
    "            'movies_count': movies_count,\n",
    "            'total_revenue': movies_with_producer['revenue'].sum(),\n",
    "            'avg_revenue': movies_with_producer['revenue'].mean(),\n",
    "            'revenue_consistency': movies_with_producer['revenue'].std(),\n",
    "            'avg_popularity': movies_with_producer['popularity'].mean(),\n",
    "            'popularity_consistency': movies_with_producer['popularity'].std(),\n",
    "            'revenue_popularity_correlation': movies_with_producer[['revenue', 'popularity']].corr().iloc[0,1]\n",
    "        }\n",
    "        producer_metrics[producer_name] = metrics\n",
    "\n",
    "    # Filter producers with minimum movies threshold\n",
    "    filtered_producer_metrics = {\n",
    "        producer: metrics \n",
    "        for producer, metrics in producer_metrics.items() \n",
    "        if metrics['movies_count'] >= min_movies\n",
    "    }\n",
    "\n",
    "    print(f\"\\nProducers before filtering: {len(producer_metrics)}\")\n",
    "    print(f\"Producers after filtering (min {min_movies} movies): {len(filtered_producer_metrics)}\")\n",
    "\n",
    "\n",
    "    # Calculate composite scores\n",
    "    for producer, metrics in producer_metrics.items():\n",
    "        revenue_norm = metrics['total_revenue'] / max(m['total_revenue'] for m in producer_metrics.values())\n",
    "        avg_norm = metrics['avg_revenue'] / max(m['avg_revenue'] for m in producer_metrics.values())\n",
    "        consistency_norm = 1 - (metrics['revenue_consistency'] / max(m['revenue_consistency'] for m in producer_metrics.values()))\n",
    "        popularity_norm = metrics['avg_popularity'] / max(m['avg_popularity'] for m in producer_metrics.values())\n",
    "        correlation_norm = abs(metrics['revenue_popularity_correlation'])\n",
    "        \n",
    "        metrics['composite_score'] = (\n",
    "            0.3 * revenue_norm +           \n",
    "            0.2 * avg_norm +               \n",
    "            0.2 * consistency_norm +       \n",
    "            0.2 * popularity_norm +        \n",
    "            0.1 * correlation_norm         \n",
    "        )\n",
    "\n",
    "    # Get top producers based on composite score\n",
    "    top_producers = sorted(producer_metrics.items(), key=lambda x: x[1]['composite_score'], reverse=True)[:n_producers]\n",
    "    top_producer_cols = [f\"crew_Producer_{producer}\" for producer, _ in top_producers]\n",
    "\n",
    "    # Process both DataFrames\n",
    "    processed_dfs = []\n",
    "    for df in [train_copy, test_copy]:\n",
    "        # Keep original non-producer columns + top producer columns\n",
    "        keep_cols = [col for col in df.columns if not col.startswith('crew_Producer_')] + top_producer_cols\n",
    "        processed = df[keep_cols].copy()\n",
    "        \n",
    "        # Add popularity weighted features for top producers\n",
    "        for producer_col in top_producer_cols:\n",
    "            if producer_col in df.columns:\n",
    "                producer_name = producer_col.replace('crew_Producer_', '')\n",
    "                processed[f\"{producer_col}_pop_weight\"] = (\n",
    "                    df[producer_col] * producer_metrics[producer_name]['avg_popularity']\n",
    "                )\n",
    "            else:\n",
    "                processed[producer_col] = 0\n",
    "                processed[f\"{producer_col}_pop_weight\"] = 0\n",
    "        \n",
    "        # Add other producer count\n",
    "        other_producer_cols = [col for col in producer_cols if col not in top_producer_cols]\n",
    "        processed['other_producer_count'] = df[other_producer_cols].sum(axis=1)\n",
    "        processed_dfs.append(processed)\n",
    "\n",
    "    # Save top producers data\n",
    "    with open('/workspace/Film_Hit_prediction/jupyter_notebooks/outputs/engineered/top_revenue_producers.pkl', 'wb') as f:\n",
    "        pickle.dump({'columns': top_producer_cols, 'metrics': producer_metrics}, f)\n",
    "\n",
    "    return processed_dfs[0], processed_dfs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting producer feature engineering...\n",
      "\n",
      "Number of producer columns found: 0\n",
      "First few producer columns: []\n",
      "\n",
      "Producers before filtering: 0\n",
      "Producers after filtering (min 10 movies): 0\n",
      "\n",
      "Feature Engineering Summary:\n",
      "--------------------\n",
      "Top producers analyzed by:\n",
      "- Total revenue\n",
      "- Average popularity\n",
      "- Revenue-popularity correlation\n",
      "\n",
      "Dataset Shapes:\n",
      "Processed train shape: (3841, 11)\n",
      "Processed test shape: (961, 11)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Call the producer function\n",
    "print(\"\\nStarting producer feature engineering...\")\n",
    "TrainSet_processed, TestSet_processed = top_revenue_producers(TrainSet, TestSet, n_producers=20)\n",
    "\n",
    "## See what the processed data looks like\n",
    "print(\"\\nFeature Engineering Summary:\")\n",
    "print(\"-\" * 20)\n",
    "print(\"Top producers analyzed by:\")\n",
    "print(\"- Total revenue\")\n",
    "print(\"- Average popularity\")\n",
    "print(\"- Revenue-popularity correlation\")\n",
    "print(\"\\nDataset Shapes:\")\n",
    "print(f\"Processed train shape: {TrainSet_processed.shape}\")\n",
    "print(f\"Processed test shape: {TestSet_processed.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAIN ENGINEERED FUNCTION \n",
    "* combining all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_movie_features(TrainSet, TestSet):\n",
    "    print(\"Starting feature engineering...\")\n",
    "    print(f\"Initial columns: {TrainSet.shape[1]}\")\n",
    "\n",
    "    # Load encoders\n",
    "    try:\n",
    "        with open('/workspace/Film_Hit_prediction/jupyter_notebooks/outputs/cleaned/encoders_and_filters.pkl', 'rb') as f:\n",
    "            encoders_and_filters = pickle.load(f)\n",
    "            print(\"Successfully loaded encoders and filters from cleaning stage\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading encoders: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "    train_processed = TrainSet.copy()  \n",
    "    test_processed = TestSet.copy()\n",
    "\n",
    "    # Get genre columns \n",
    "    genre_columns = ['Action', 'Adventure', 'Animation', 'Comedy', 'Crime', \n",
    "                    'Documentary', 'Drama', 'Family', 'Fantasy', 'History', \n",
    "                    'Horror', 'Music', 'Mystery', 'Romance', 'Science Fiction', \n",
    "                    'TV Movie', 'Thriller', 'War', 'Western']\n",
    "    print(f\"Using {len(genre_columns)} genre columns: {genre_columns}\")\n",
    "\n",
    "    # Remove movies with missing revenue\n",
    "    train_processed = train_processed.dropna(subset=['revenue','runtime','budget'])\n",
    "    test_processed = test_processed.dropna(subset=['revenue','runtime','budget'])\n",
    "\n",
    "    # 1. BUDGET FEATURES\n",
    "    print(\"\\nEngineering budget features...\")\n",
    "    \n",
    "    # First create budget_per_minute BEFORE trying to scale it\n",
    "    train_processed['budget_per_minute'] = (train_processed['budget'] / train_processed['runtime'].replace(0, 1)).fillna(0)\n",
    "    test_processed['budget_per_minute'] = test_processed['budget'] / test_processed['runtime'].replace(0, 1)\n",
    "    \n",
    "    # Now identify numerical columns to scale\n",
    "    numeric_cols = [\n",
    "        'budget', \n",
    "        'runtime', \n",
    "        'popularity',\n",
    "        'budget_per_minute',  # Now this column exists\n",
    "    ]\n",
    "\n",
    "    # 2. RUNTIME FEATURES\n",
    "    print(\"Engineering runtime features...\")\n",
    "    train_processed = train_processed[train_processed['runtime'] >= 90]\n",
    "    test_processed = test_processed[test_processed['runtime'] >= 90]\n",
    "\n",
    "    # 3. CAST/CREW FEATURES\n",
    "    train_processed, test_processed = top_revenue_actors(train_processed, test_processed, n_actors=30)\n",
    "    train_processed, test_processed = top_revenue_directors(train_processed, test_processed, n_directors=20)\n",
    "    train_processed, test_processed = top_revenue_writers(train_processed, test_processed, n_writers=10)\n",
    "    train_processed, test_processed = top_revenue_producers(train_processed, test_processed, n_producers=20)\n",
    "\n",
    "    # Add popularity weight columns to numeric_cols AFTER crew features are added\n",
    "    pop_weight_cols = [col for col in train_processed.columns if col.endswith('_pop_weight')]\n",
    "    numeric_cols.extend(pop_weight_cols)\n",
    "\n",
    "    # 4. LANGUAGE FEATURES\n",
    "    english_code = encoders_and_filters['language_encoder'].transform(['en'])[0]\n",
    "    train_processed['is_english'] = (train_processed['language_encoded'] == english_code).astype(int)\n",
    "    test_processed['is_english'] = (test_processed['language_encoded'] == english_code).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "    # Before scaling, check for problematic columns\n",
    "    print(\"\\nChecking for problematic columns before scaling:\")\n",
    "    columns_to_exclude = []\n",
    "    for col in train_processed.columns: \n",
    "        unique_vals = train_processed[col].nunique()\n",
    "        has_nan = train_processed[col].isna().any()\n",
    "        try:\n",
    "            variance = train_processed[col].var()\n",
    "        except:\n",
    "            variance = 0\n",
    "        if unique_vals == 1 or has_nan or variance == 0:\n",
    "            print(f\"\\nPotentially problematic column: {col}\")\n",
    "            print(f\"Unique values: {unique_vals}\")\n",
    "            print(f\"Has NaN: {has_nan}\")\n",
    "            print(f\"Variance: {variance}\")\n",
    "            if col in numeric_cols:\n",
    "                columns_to_exclude.append(col)\n",
    "\n",
    "    for col in numeric_cols:\n",
    "        if col in train_processed.columns:\n",
    "            train_processed[col] = train_processed[col].fillna(0)\n",
    "            test_processed[col] = test_processed[col].fillna(0)\n",
    "\n",
    "    columns_to_exclude = []\n",
    "    # ... check for problematic columns ...\n",
    "    numeric_cols = [col for col in numeric_cols if col not in columns_to_exclude]\n",
    "\n",
    "    \n",
    "    # 5. SCALING - now all columns exist before scaling\n",
    "    print(\"Scaling numerical features...\")\n",
    "    scaler = StandardScaler()\n",
    "    train_processed[numeric_cols] = scaler.fit_transform(train_processed[numeric_cols])\n",
    "    test_processed[numeric_cols] = scaler.transform(test_processed[numeric_cols])\n",
    "\n",
    "    # Save the scaler\n",
    "    with open('/workspace/Film_Hit_prediction/jupyter_notebooks/outputs/engineered/feature_scaler.pkl', 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "\n",
    "    # Save all transformation data\n",
    "    transformation_data = {\n",
    "        'feature_scaler': scaler,\n",
    "        'encoders_and_filters': encoders_and_filters,\n",
    "        'numeric_cols': numeric_cols,\n",
    "        'genre_columns': genre_columns,\n",
    "        'all_features': list(train_processed.columns),\n",
    "        'train_stats': {\n",
    "            'budget_mean': TrainSet['budget'].mean(),\n",
    "            'budget_std': TrainSet['budget'].std(),\n",
    "            'revenue_mean': TrainSet['revenue'].mean(),\n",
    "            'revenue_std': TrainSet['revenue'].std()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open('/workspace/Film_Hit_prediction/jupyter_notebooks/outputs/engineered/full_transformation_data.pkl', 'wb') as f:\n",
    "        pickle.dump(transformation_data, f)\n",
    "\n",
    "    # Rest of your function remains the same...\n",
    "    \n",
    "    return train_processed , test_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting feature engineering...\n",
      "Initial columns: 10\n",
      "Successfully loaded encoders and filters from cleaning stage\n",
      "Using 19 genre columns: ['Action', 'Adventure', 'Animation', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Family', 'Fantasy', 'History', 'Horror', 'Music', 'Mystery', 'Romance', 'Science Fiction', 'TV Movie', 'Thriller', 'War', 'Western']\n",
      "\n",
      "Engineering budget features...\n",
      "Engineering runtime features...\n",
      "Number of columns after adding top actor features: 71\n",
      "Number of director columns found: 0\n",
      "First few director columns: []\n",
      "\n",
      "Number of writer columns found: 0\n",
      "First few writer columns: []\n",
      "\n",
      "Found 0 writers\n",
      "\n",
      "All writers and their metrics:\n",
      "No writers found.\n",
      "\n",
      "Number of producer columns found: 0\n",
      "First few producer columns: []\n",
      "\n",
      "Producers before filtering: 0\n",
      "Producers after filtering (min 10 movies): 0\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'language_encoded'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/workspace/.pip-modules/lib/python3.8/site-packages/pandas/core/indexes/base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3620\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/workspace/.pip-modules/lib/python3.8/site-packages/pandas/_libs/index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/workspace/.pip-modules/lib/python3.8/site-packages/pandas/_libs/index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'language_encoded'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mengineer_movie_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTrainSet\u001b[49m\u001b[43m,\u001b[49m\u001b[43mTestSet\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 60\u001b[0m, in \u001b[0;36mengineer_movie_features\u001b[0;34m(TrainSet, TestSet)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# 4. LANGUAGE FEATURES\u001b[39;00m\n\u001b[1;32m     59\u001b[0m english_code \u001b[38;5;241m=\u001b[39m encoders_and_filters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlanguage_encoder\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtransform([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m])[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 60\u001b[0m train_processed[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_english\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (\u001b[43mtrain_processed\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlanguage_encoded\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m english_code)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m     61\u001b[0m test_processed[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_english\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (test_processed[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlanguage_encoded\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m english_code)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Before scaling, check for problematic columns\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/.pip-modules/lib/python3.8/site-packages/pandas/core/frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3505\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3507\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/workspace/.pip-modules/lib/python3.8/site-packages/pandas/core/indexes/base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3623\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3624\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3625\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3626\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3627\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'language_encoded'"
     ]
    }
   ],
   "source": [
    "engineer_movie_features(TrainSet,TestSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering Spreadsheet Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Languages are properly encoded using LabelEncoder\n",
    "- Genre columnes are already one-hot encoded\n",
    "- Budget is both log- transformed and scaled\n",
    "- Saved the encoders and scalers\n",
    "- Target variable (revenue) is Lon-transformed to handle skewness and scaled using StandardScaler\n",
    "- Processed datasets are saved.\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PUSH TO REPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save as one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting feature engineering process...\n",
      "Starting feature engineering...\n",
      "Initial columns: 1131\n",
      "Successfully loaded encoders and filters from cleaning stage\n",
      "Using 19 genre columns: ['Action', 'Adventure', 'Animation', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Family', 'Fantasy', 'History', 'Horror', 'Music', 'Mystery', 'Romance', 'Science Fiction', 'TV Movie', 'Thriller', 'War', 'Western']\n",
      "\n",
      "Engineering budget features...\n",
      "Engineering runtime features...\n",
      "Number of columns after adding top actor features: 1192\n",
      "Number of director columns found: 227\n",
      "First few director columns: ['crew_Director_Aaron Seltzer', 'crew_Director_Adam McKay', 'crew_Director_Adam Shankman', 'crew_Director_Alejandro González Iñárritu', 'crew_Director_Alex Proyas']\n",
      "\n",
      "Number of writer columns found: 11\n",
      "First few writer columns: ['crew_Writer_David Zucker', 'crew_Writer_Ethan Coen', 'crew_Writer_Joel Coen', 'crew_Writer_Kevin Smith', 'crew_Writer_Luc Besson']\n",
      "\n",
      "Found 11 writers\n",
      "\n",
      "All writers and their metrics:\n",
      "David Zucker: 0 movies, $0.00 total revenue\n",
      "Ethan Coen: 3 movies, $128,013,309.00 total revenue\n",
      "Joel Coen: 3 movies, $128,013,309.00 total revenue\n",
      "Kevin Smith: 4 movies, $76,707,267.00 total revenue\n",
      "Luc Besson: 3 movies, $150,644,583.00 total revenue\n",
      "M. Night Shyamalan: 5 movies, $1,904,372,773.00 total revenue\n",
      "Mike Leigh: 4 movies, $23,529,762.00 total revenue\n",
      "Quentin Tarantino: 4 movies, $504,229,064.00 total revenue\n",
      "Robert Rodriguez: 3 movies, $98,763,863.00 total revenue\n",
      "Tyler Perry: 4 movies, $147,739,860.00 total revenue\n",
      "Woody Allen: 5 movies, $358,668,130.00 total revenue\n",
      "\n",
      "Top writers by composite score:\n",
      "David Zucker:\n",
      "  Movies: 0\n",
      "  Total Revenue: $0.00\n",
      "  Avg Revenue: $nan\n",
      "  Composite Score: nan\n",
      "M. Night Shyamalan:\n",
      "  Movies: 5\n",
      "  Total Revenue: $1,904,372,773.00\n",
      "  Avg Revenue: $380,874,554.60\n",
      "  Composite Score: 0.384\n",
      "Quentin Tarantino:\n",
      "  Movies: 4\n",
      "  Total Revenue: $504,229,064.00\n",
      "  Avg Revenue: $126,057,266.00\n",
      "  Composite Score: 0.170\n",
      "Woody Allen:\n",
      "  Movies: 5\n",
      "  Total Revenue: $358,668,130.00\n",
      "  Avg Revenue: $71,733,626.00\n",
      "  Composite Score: 0.145\n",
      "Robert Rodriguez:\n",
      "  Movies: 3\n",
      "  Total Revenue: $98,763,863.00\n",
      "  Avg Revenue: $32,921,287.67\n",
      "  Composite Score: 0.111\n",
      "Ethan Coen:\n",
      "  Movies: 3\n",
      "  Total Revenue: $128,013,309.00\n",
      "  Avg Revenue: $42,671,103.00\n",
      "  Composite Score: 0.087\n",
      "Joel Coen:\n",
      "  Movies: 3\n",
      "  Total Revenue: $128,013,309.00\n",
      "  Avg Revenue: $42,671,103.00\n",
      "  Composite Score: 0.087\n",
      "Kevin Smith:\n",
      "  Movies: 4\n",
      "  Total Revenue: $76,707,267.00\n",
      "  Avg Revenue: $19,176,816.75\n",
      "  Composite Score: 0.079\n",
      "Tyler Perry:\n",
      "  Movies: 4\n",
      "  Total Revenue: $147,739,860.00\n",
      "  Avg Revenue: $36,934,965.00\n",
      "  Composite Score: 0.054\n",
      "Luc Besson:\n",
      "  Movies: 3\n",
      "  Total Revenue: $150,644,583.00\n",
      "  Avg Revenue: $50,214,861.00\n",
      "  Composite Score: 0.040\n",
      "\n",
      "Number of producer columns found: 462\n",
      "First few producer columns: ['crew_Producer_A. Kitman Ho', 'crew_Producer_Aaron Ryder', 'crew_Producer_Adam McKay', 'crew_Producer_Adam Sandler', 'crew_Producer_Adam Shankman']\n",
      "\n",
      "Producers before filtering: 462\n",
      "Producers after filtering (min 10 movies): 65\n",
      "\n",
      "Checking for problematic columns before scaling:\n",
      "\n",
      "Potentially problematic column: crew_Director_Aaron Seltzer\n",
      "Unique values: 1\n",
      "Has NaN: False\n",
      "Variance: 0.0\n",
      "\n",
      "Potentially problematic column: crew_Director_Aaron Seltzer_pop_weight\n",
      "Unique values: 0\n",
      "Has NaN: True\n",
      "Variance: nan\n",
      "\n",
      "Potentially problematic column: crew_Writer_David Zucker\n",
      "Unique values: 1\n",
      "Has NaN: False\n",
      "Variance: 0.0\n",
      "\n",
      "Potentially problematic column: crew_Writer_David Zucker_pop_weight\n",
      "Unique values: 0\n",
      "Has NaN: True\n",
      "Variance: nan\n",
      "Scaling numerical features...\n",
      "\n",
      "Feature Engineering Results:\n",
      "Final TrainSet shape: (3284, 270)\n",
      "Final TestSet shape: (813, 270)\n",
      "\n",
      "Saving processed datasets...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run feature engineering\n",
    "print(\"Starting feature engineering process...\")\n",
    "train_processed, test_processed = engineer_movie_features(TrainSet, TestSet)\n",
    "\n",
    "if train_processed is not None and test_processed is not None:\n",
    "    print(\"\\nFeature Engineering Results:\")\n",
    "    print(f\"Final TrainSet shape: {train_processed.shape}\")\n",
    "    print(f\"Final TestSet shape: {test_processed.shape}\")\n",
    "    \n",
    "    # Save the processed datasets\n",
    "    print(\"\\nSaving processed datasets...\")\n",
    "    train_processed.to_pickle('/workspace/Film_Hit_prediction/jupyter_notebooks/outputs/engineered/train_df_engineered.pkl')\n",
    "    test_processed.to_pickle('/workspace/Film_Hit_prediction/jupyter_notebooks/outputs/engineered/test_df_engineered.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into X and Y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final shapes after loading:\n",
      "TrainSet shape: (3284, 270)\n",
      "TestSet shape: (813, 270)\n",
      "\n",
      "Number of total feature columns: 269\n",
      "\n",
      "Shapes after splitting:\n",
      "X_train shape: (3284, 269)\n",
      "y_train shape: (3284,)\n",
      "X_test shape: (813, 269)\n",
      "y_test shape: (813,)\n",
      "\n",
      "Saving splits...\n",
      "Splits saved successfully!\n",
      "Training data shape: (3284, 270)\n",
      "Test data shape: (813, 270)\n",
      "\n",
      "Feature engineering completed!\n"
     ]
    }
   ],
   "source": [
    "# Load the saved processed datasets\n",
    "train_processed = pd.read_pickle('/workspace/Film_Hit_prediction/jupyter_notebooks/outputs/engineered/train_df_engineered.pkl')\n",
    "test_processed = pd.read_pickle('/workspace/Film_Hit_prediction/jupyter_notebooks/outputs/engineered/test_df_engineered.pkl')\n",
    "\n",
    "print(\"\\nFinal shapes after loading:\")\n",
    "print(f\"TrainSet shape: {train_processed.shape}\")\n",
    "print(f\"TestSet shape: {test_processed.shape}\")\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "feature_columns = list(set([col for col in train_processed.columns if col != 'revenue']))\n",
    "\n",
    "print(f\"\\nNumber of total feature columns: {len(feature_columns)}\")\n",
    "\n",
    "X_train = train_processed[feature_columns]\n",
    "y_train = train_processed['revenue']\n",
    "\n",
    "X_test = test_processed[feature_columns]\n",
    "y_test = test_processed['revenue']\n",
    "\n",
    "print(f\"\\nShapes after splitting:\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Save the splits in the engineered directory\n",
    "output_dir = '/workspace/Film_Hit_prediction/jupyter_notebooks/outputs/engineered'\n",
    "\n",
    "print(\"\\nSaving splits...\")\n",
    "X_train.to_pickle(f'{output_dir}/X_train.pkl')\n",
    "X_test.to_pickle(f'{output_dir}/X_test.pkl')\n",
    "y_train.to_pickle(f'{output_dir}/y_train.pkl')\n",
    "y_test.to_pickle(f'{output_dir}/y_test.pkl')\n",
    "\n",
    "print(\"Splits saved successfully!\")\n",
    "\n",
    "print(f\"Training data shape: {train_processed.shape}\")\n",
    "print(f\"Test data shape: {test_processed.shape}\")\n",
    "print(\"\\nFeature engineering completed!\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved processed datasets\n",
    "train_processed = pd.read_pickle('/workspace/Film_Hit_prediction/jupyter_notebooks/outputs/engineered/test_df_engineered.pkl')\n",
    "test_processed = pd.read_pickle('/workspace/Film_Hit_prediction/jupyter_notebooks/outputs/engineered/train_df_engineered.pkl')\n",
    "\n",
    "print(\"Column names sample:\")\n",
    "print(list(train_processed.columns)[:10])  # Print first 10 column names\n",
    "\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "feature_columns = [col for col in train_processed.columns if col != 'revenue']\n",
    "\n",
    "X_train = train_processed[feature_columns]\n",
    "y_train = train_processed['revenue']\n",
    "\n",
    "X_test = test_processed[feature_columns]\n",
    "y_test = test_processed['revenue']\n",
    "\n",
    "\n",
    "# Save the splits in the engineered directory\n",
    "output_dir = '/workspace/Film_Hit_prediction/jupyter_notebooks/outputs/engineered'\n",
    "\n",
    "# Print the shapes of the final datasets\n",
    "print(\"\\nDataset shapes:\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Print a few sample rows from the datasets to inspect\n",
    "print(\"\\nSample rows from X_train:\")\n",
    "print(X_train.head())\n",
    "\n",
    "print(\"\\nSample rows from y_train:\")\n",
    "print(y_train.head())\n",
    "\n",
    "print(\"\\nSample rows from X_test:\")\n",
    "print(X_test.head())\n",
    "\n",
    "print(\"\\nSample rows from y_test:\")\n",
    "print(y_test.head())\n",
    "\n",
    "# Print the list of final features\n",
    "print(\"\\nFeatures included in the final dataset:\")\n",
    "for feature in sorted(feature_columns):\n",
    "    print(f\"- {feature}\")\n",
    "\n",
    "print(\"\\nFeature engineering completed!\")\n",
    "print(f\"Training data shape: {train_processed.shape}\")\n",
    "print(f\"Test data shape: {test_processed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of unique feature columns: {len(set(feature_columns))}\")\n",
    "print(f\"Number of feature columns: {len(feature_columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate columns\n",
    "print(f\"Number of unique columns in TrainSet: {len(train_processed.columns.unique())}\")\n",
    "print(f\"Number of columns in TrainSet: {len(train_processed.columns)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
